{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 시작"
      ],
      "metadata": {
        "id": "y1EaD9Ciq3nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['LANGSMITH_TRACING'] = userdata.get('LANGSMITH_TRACING')\n",
        "os.environ['LANGSMITH_ENDPOINT'] = userdata.get('LANGSMITH_ENDPOINT')\n",
        "os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ['LANGSMITH_PROJECT'] = userdata.get('LANGSMITH_PROJECT')\n",
        "os.environ['COHERE_API_KEY'] = userdata.get('COHERE_API_KEY')\n",
        "os.environ['TAVILY_API_KEY'] = userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "UEnSMBwTDVb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangSmith를 활용한 RAG 애플리케이션 평가"
      ],
      "metadata": {
        "id": "08m3Agan-ON7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-core==0.2.30 langchain-openai==0.1.21 langchain-community==0.2.12 GitPython==3.1.43 langchain-chroma==0.1.2 chromadb==0.5.3 ragas==0.1.4 nest-asyncio==1.6.0 pydantic==2.9.2 numpy==1.26.0"
      ],
      "metadata": {
        "id": "Ym19o626-g4H",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import GitLoader\n",
        "\n",
        "def file_filter(file_path: str) -> bool:\n",
        "    return file_path.endswith(\".md\")\n",
        "\n",
        "loader = GitLoader(\n",
        "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
        "    repo_path=\"./langchain\",\n",
        "    branch=\"master\",\n",
        "    file_filter=file_filter,\n",
        ")\n",
        "\n",
        "documents = loader.load()\n",
        "print(f\"로드된 문서 개수: {len(documents)}\")"
      ],
      "metadata": {
        "id": "sTHwTRlXooay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for document in documents:\n",
        "  document.metadata[\"filename\"] = document.metadata[\"source\"]"
      ],
      "metadata": {
        "id": "LaBZWAK8o-aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "generator = TestsetGenerator.from_langchain(\n",
        "    generator_llm=ChatOpenAI(model=\"gpt-4o\"),\n",
        "    critic_llm=ChatOpenAI(model=\"gpt-4o\"),\n",
        "    embeddings=OpenAIEmbeddings(),\n",
        ")\n",
        "\n",
        "testset = generator.generate_with_langchain_docs(\n",
        "    documents,\n",
        "    test_size=4,\n",
        "    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
        ")"
      ],
      "metadata": {
        "id": "AR3g8icEpaC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testset.to_pandas()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6RVwAkSfq_c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "dataset_name = \"agent-book\"\n",
        "client = Client()\n",
        "\n",
        "if client.has_dataset(dataset_name=dataset_name):\n",
        "  client.delete_dataset(dataset_name=dataset_name)\n",
        "\n",
        "dataset = client.create_dataset(dataset_name=dataset_name)"
      ],
      "metadata": {
        "id": "Yv4yewU2re7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.utils import metadata\n",
        "inputs = []\n",
        "outputs = []\n",
        "metadatas = []\n",
        "\n",
        "for testset_record in testset.test_data:\n",
        "  inputs.append({\n",
        "      \"question\": testset_record.question,\n",
        "  })\n",
        "  outputs.append({\n",
        "      \"contexts\": testset_record.contexts,\n",
        "      \"ground_truth\": testset_record.ground_truth,\n",
        "  })\n",
        "  metadatas.append({\n",
        "      \"evolution_type\": testset_record.evolution_type,\n",
        "  })"
      ],
      "metadata": {
        "id": "znwvhkkGt3x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.create_examples(\n",
        "    dataset_id=dataset.id,\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    metadatas=metadatas,\n",
        ")"
      ],
      "metadata": {
        "id": "okXpVJKiwgzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langsmith.schemas import Example, Run\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics.base import Metric, MetricWithEmbeddings, MetricWithLLM\n",
        "\n",
        "class RagasMetricEvaluator:\n",
        "  def __init__(self, metric: Metric, llm: BaseChatModel, embeddings: Embeddings):\n",
        "    self.metric = metric\n",
        "\n",
        "    # LLM과 Embeddings를 Metric에 설정\n",
        "    if isinstance(self.metric, MetricWithLLM):\n",
        "      self.metric.llm = LangchainLLMWrapper(llm)\n",
        "    if isinstance(self.metric, MetricWithEmbeddings):\n",
        "      self.metric.embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
        "\n",
        "  def evaluate(self, example: Example, run: Run) -> dict[str, Any]:\n",
        "    context_strs = [doc.page_content for doc in run.outputs[\"contexts\"]]\n",
        "\n",
        "    # Ragas의 평가 메트릭의 score 메서드로 점수 계산\n",
        "    score = self.metric.score(\n",
        "        {\n",
        "            \"question\": example.inputs[\"question\"],\n",
        "            \"answer\": run.outputs[\"answer\"],\n",
        "            \"contexts\": context_strs,\n",
        "            \"ground_truth\": example.outputs[\"ground_truth\"],\n",
        "        },\n",
        "    )\n",
        "\n",
        "    return {\"key\": self.metric.name, \"score\": score}\n"
      ],
      "metadata": {
        "id": "5zsjd1NNdRtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from ragas.metrics import answer_relevancy, context_precision\n",
        "\n",
        "metrics = [context_precision, answer_relevancy]\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "evaluators = [\n",
        "    RagasMetricEvaluator(metric, llm, embeddings).evaluate\n",
        "    for metric in metrics\n",
        "]"
      ],
      "metadata": {
        "id": "-KUDkBDhggx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "db = Chroma.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "Q9SHqIaphT0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "db = Chroma.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "V4EYg2m7iT6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template('''\n",
        "다음 문맥만을 고려해 질문에 대답하세요.\n",
        "\n",
        "문맥: \"\"\"{context}\"\"\"\n",
        "질문: {question}\n",
        "''')\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "chain = RunnableParallel(\n",
        "    {\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        \"context\": retriever,\n",
        "    }\n",
        ").assign(\n",
        "    answer=prompt | model | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "j8wf5hQzih3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(inputs: dict[str, Any]) -> dict[str, Any]:\n",
        "  question = inputs[\"question\"]\n",
        "  output = chain.invoke(question)\n",
        "  return {\n",
        "      \"contexts\": output[\"context\"],\n",
        "      \"answer\": output[\"answer\"],\n",
        "  }"
      ],
      "metadata": {
        "id": "e_-lSzr8jUTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith.evaluation import evaluate\n",
        "\n",
        "evaluate(\n",
        "    predict,\n",
        "    data=\"agent-book\",\n",
        "    evaluators=evaluators,\n",
        ")"
      ],
      "metadata": {
        "id": "udr2LkDBjodp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import UUID\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from langsmith import Client\n",
        "\n",
        "def display_feedback_buttons(run_id: UUID) -> None:\n",
        "  # Good 버튼과 Bad 버튼 준비\n",
        "  good_button = widgets.Button(description=\"Good\",button_style=\"success\",icon=\"thumbs-up\")\n",
        "  bad_button = widgets.Button(description=\"Bad\",button_style=\"danger\",icon=\"thumbs-down\")\n",
        "\n",
        "  # 클릭됐을 때 실행되는 함수 정의\n",
        "  def on_button_clicked(button: widgets.Button) -> None:\n",
        "    if button == good_button:\n",
        "      score = 1\n",
        "    elif button == bad_button:\n",
        "      score = 0\n",
        "    else:\n",
        "      raise ValueError(f\"Invalid button: {button}\")\n",
        "\n",
        "    client = Client()\n",
        "    client.create_feedback(run_id=run_id, key=\"thumbs\", score=score)\n",
        "    print(\"피드백을 전송 했습니다.\")\n",
        "\n",
        "  good_button.on_click(on_button_clicked)\n",
        "  bad_button.on_click(on_button_clicked)\n",
        "\n",
        "  display(good_button, bad_button)"
      ],
      "metadata": {
        "id": "ZHKulEmBrxQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tracers.context import collect_runs\n",
        "\n",
        "# LangSmith의 트레이스 ID(Run ID)를 얻기 위해 collect_runs 함수 사용\n",
        "with collect_runs() as runs_cb:\n",
        "  output = chain.invoke(\"LangChain의 개요를 알려줘\")\n",
        "  print(output[\"answer\"])\n",
        "  run_id = runs_cb.traced_runs[0].id\n",
        "\n",
        "display_feedback_buttons(run_id)"
      ],
      "metadata": {
        "id": "EE9xgsePtHGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced RAG"
      ],
      "metadata": {
        "id": "RiKMRglzppcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-core==0.3.0 langchain-openai==0.2.0 langchain-community==0.3.0 GitPython==3.1.43 langchain-chroma==0.1.4 tavily-python==0.5.0"
      ],
      "metadata": {
        "id": "FfbwEKr-p8dN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import GitLoader\n",
        "\n",
        "def file_filter(file_path: str) -> bool:\n",
        "    return file_path.endswith(\".md\")\n",
        "\n",
        "loader = GitLoader(\n",
        "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
        "    repo_path=\"./langchain\",\n",
        "    branch=\"master\",\n",
        "    file_filter=file_filter,\n",
        ")\n",
        "documents = loader.load()\n",
        "print(f\"로드된 문서 개수: {len(documents)}\")"
      ],
      "metadata": {
        "id": "5sZDxAN7psy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "db = Chroma.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fj2GVLsSrFVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template('''\n",
        "다음 문맥만을 고려해 질문에 대답하세요.\n",
        "\n",
        "문맥: \"\"\"\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "질문: {question}\n",
        "'''\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"context\": retriever,\n",
        "} | prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke(\"LangChain의 개요를 알려줘\")"
      ],
      "metadata": {
        "id": "3kjx8qcbrRB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class QueryGenerationOutput(BaseModel):\n",
        "    queries: list[str] = Field(..., description=\"검색 쿼리 목록\")\n",
        "\n",
        "query_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "질문에 대해 벡터 데이터베이스에서 관련 문서를 검색하기 위한\n",
        "3개의 서로 다른 검색 쿼리를 생성하세요.\n",
        "거리 기반 유사성 검색의 한계를 극복하기 위해\n",
        "사용자의 질문에 대해 여러 관점을 제공하는 것이 목표입니다.\n",
        "\n",
        "질문: {question}\n",
        "\"\"\")\n",
        "\n",
        "query_generation_chain = (\n",
        "    query_generation_prompt\n",
        "    | model.with_structured_output(QueryGenerationOutput)\n",
        "    | (lambda x: x.queries)\n",
        ")\n",
        "\n",
        "multi_query_rag_chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"context\": query_generation_chain | retriever.map(),\n",
        "} | prompt | model | StrOutputParser()\n",
        "\n",
        "multi_query_rag_chain.invoke(\"LangChain의 개요를 알려줘\")"
      ],
      "metadata": {
        "id": "Yjjab4VurZV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "def reciprocal_rank_fusion(\n",
        "    retriever_outputs: list[list[Document]],\n",
        "    k: int = 60,\n",
        ") -> list[str]:\n",
        "    # 각 문서의 콘텐츠(문자열)와 그 점수의 매핑을 저장하는 딕셔너리 준비\n",
        "    content_score_mapping = {}\n",
        "\n",
        "    # 검색 쿼리마다 반복\n",
        "    for docs in retriever_outputs:\n",
        "        # 검색 결과의 문서마다 반복\n",
        "        for rank, doc in enumerate(docs):\n",
        "            content = doc.page_content\n",
        "\n",
        "            # 처음 등장한 콘텐츠인 경우 점수를 0으로 초기화\n",
        "            if content not in content_score_mapping:\n",
        "                content_score_mapping[content] = 0\n",
        "\n",
        "            # (1 / (순위 + k)) 점수를 추가\n",
        "            content_score_mapping[content] += 1\n",
        "\n",
        "    # 점수가 큰 순서로 정렬\n",
        "    ranked = sorted(content_score_mapping.items(),key=lambda x: x[1], reverse=True) # nopa\n",
        "    return [content for content,_ in ranked]\n",
        "\n",
        "rag_fusion_chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"context\": query_generation_chain | retriever.map() | reciprocal_rank_fusion,\n",
        "} | prompt | model | StrOutputParser()\n",
        "\n",
        "rag_fusion_chain.invoke(\"Langchain의 개요를 알려줘\")\n"
      ],
      "metadata": {
        "id": "AKtPlxA1rdgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-cohere==0.3.0"
      ],
      "metadata": {
        "id": "Vs-DuabWzt-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def rerank(inp: dict[str, Any], top_n: int =3 ) -> list[Document]:\n",
        "  question = inp[\"question\"]\n",
        "  documents = inp[\"documents\"]\n",
        "\n",
        "  cohere_reranker = CohereRerank(model=\"rerank-multilingual-v3.0\",top_n=top_n)\n",
        "  return cohere_reranker.compress_documents(documents=documents, query= question)\n",
        "\n",
        "rerank_rag_chain = (\n",
        "    {\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        \"documents\": retriever,\n",
        "    }\n",
        "    | RunnablePassthrough.assign(context=rerank)\n",
        "    | prompt | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "rerank_rag_chain.invoke(\"Langchain의 개요를 알려줘\")"
      ],
      "metadata": {
        "id": "nuW97redr6ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
        "\n",
        "langchain_document_retrierver = retriever.with_config({\"run_name\": \"langchain_document_retrierver\"})\n",
        "web_retriever = TavilySearchAPIRetriever(k=3).with_config({\"run_name\": \"web_retriever\"})"
      ],
      "metadata": {
        "id": "AX-V12qOvIh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "class Route(str, Enum):\n",
        "  langchain_document = \"langchain_document\"\n",
        "  web = \"web\"\n",
        "\n",
        "class RouteOutput(BaseModel):\n",
        "  route: Route\n",
        "\n",
        "route_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "질문에 답변하기 위해 적절한 Retriever를 선택하세요.\n",
        "\n",
        "질문: {question}\n",
        "\"\"\")\n",
        "\n",
        "route_chain = (\n",
        "    route_prompt\n",
        "    | model.with_structured_output(RouteOutput)\n",
        "    | (lambda x: x.route)\n",
        ")"
      ],
      "metadata": {
        "id": "C0K6FLZvvjNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def routed_retriever(inp: dict[str, Any]) -> list[Document]:\n",
        "  question = inp[\"question\"]\n",
        "  route = inp[\"route\"]\n",
        "\n",
        "  if route == Route.langchain_document:\n",
        "    return langchain_document_retrierver.invoke(question)\n",
        "  elif route == Route.web:\n",
        "    return web_retriever.invoke(question)\n",
        "\n",
        "  raise ValueError(f\"Invalid route: {route}\")\n",
        "\n",
        "route_rag_chain = (\n",
        "    {\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        \"route\": route_chain,\n",
        "    } | RunnablePassthrough.assign(context=routed_retriever)\n",
        "    | prompt | model | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "FekRXvuAwFcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "route_rag_chain.invoke(\"Langchain의 개요를 알려줘\")"
      ],
      "metadata": {
        "id": "DGiL2hJ9wgTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "route_rag_chain.invoke(\"오늘 날씨는 어때?\")"
      ],
      "metadata": {
        "id": "rPIH6XSnxFZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank-bm25==0.2.2"
      ],
      "metadata": {
        "id": "nTICwjHfzgTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "chroma_retriever = retriever.with_config({\"run_name\": \"chroma_retriever\"})\n",
        "bm25_retriever = BM25Retriever.from_documents(documents).with_config({\"run_name\": \"bm25_retriever\"})"
      ],
      "metadata": {
        "id": "dXuWRitezx31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "hybrid_retriever = RunnableParallel(\n",
        "    {\"bm25_documents\": bm25_retriever, \"chroma_documents\": chroma_retriever}\n",
        ") | (lambda x: [x[\"bm25_documents\"], x[\"chroma_documents\"]]) | reciprocal_rank_fusion"
      ],
      "metadata": {
        "id": "xIeU28FF0Cx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_rag_chain = (\n",
        "    {\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        \"context\": hybrid_retriever,\n",
        "    }\n",
        "    | prompt | model | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "sA12Jn_U0diO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_rag_chain.invoke(\"Langchain의 개요를 알려줘\")"
      ],
      "metadata": {
        "id": "ryvQDaGg0i5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기초"
      ],
      "metadata": {
        "id": "mbI2GZJ7yD7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N0QewXPJB5z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SxCZwGq0o_vo"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "model = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "output = model.invoke(\"안녕하세요.\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"You are a helpful assistant.\"),\n",
        "    HumanMessage(\"안녕하세요! 저는 존이라고 합니다.\"),\n",
        "    AIMessage(content=\"안녕하세요, 존님! 어떤 도움이 필요하신각요?\"),\n",
        "    HumanMessage(content=\"제 이름을 아시나요?\"),\n",
        "]\n",
        "\n",
        "ai_message = model.invoke(messages)\n",
        "print(ai_message.content)"
      ],
      "metadata": {
        "id": "3zau8XLzEpje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"You are a helpful assistant.\"),\n",
        "    HumanMessage(\"안녕하세요!\"),\n",
        "]\n",
        "\n",
        "for chunk in model.stream(messages):\n",
        "  print(chunk.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "bRzckglhFpSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"\"\"다음 요리의 레시피를 생각해 주세요.\n",
        "\n",
        "요리명: {dish}\"\"\")\n",
        "\n",
        "prompt_value = prompt.invoke({\"dish\": \"김치볶음밥\"})\n",
        "print(prompt_value.text)"
      ],
      "metadata": {
        "id": "kFmFzrBBG0-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\"),\n",
        "        (\"human\",\"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_value = prompt.invoke({\"dish\":\"카레\"})\n",
        "print(prompt_value)"
      ],
      "metadata": {
        "id": "iOUi7192Hhiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\"),\n",
        "        MessagesPlaceholder(\"chat_history\",optional=True),\n",
        "        (\"human\",\"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_value = prompt.invoke(\n",
        "    {\n",
        "        \"chat_history\": [\n",
        "            HumanMessage(content=\"카레는 어떤 음식인가요?\"),\n",
        "            AIMessage(\"안녕하세요, 존님! 어떻게 도와드릴까요?\"),\n",
        "        ],\n",
        "        \"input\":\"제 이름은 아시나요?\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(prompt_value)"
      ],
      "metadata": {
        "id": "xjnXYU_iIIMz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}